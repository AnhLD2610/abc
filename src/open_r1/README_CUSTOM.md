# Efficient Reasoning v·ªõi Enhanced GRPO

ƒê√¢y l√† implementation c·ªßa c√°c contribute ƒë·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ blackbox GRPO trainer v√† c·∫£i thi·ªán efficient reasoning.

## ‚ú® T√≠nh NƒÉng Ch√≠nh

### 1. Enhanced GRPO Trainer v·ªõi Group-wise Contrastive Learning

**V·∫•n ƒë·ªÅ**: GRPO trainer g·ªëc l√† blackbox kh√¥ng th·ªÉ modify tr·ª±c ti·∫øp.

**Gi·∫£i ph√°p**: T·∫°o `EnhancedGRPOTrainer` k·∫ø th·ª´a ho√†n to√†n t·ª´ `GRPOTrainer` v·ªõi:

- **üîÑ Backward Compatible**: Ho√†n to√†n t∆∞∆°ng th√≠ch v·ªõi GRPO g·ªëc
- **üéØ Group-wise Contrastive**: Contrastive learning trong t·ª´ng nh√≥m N generations t·ª´ c√πng 1 prompt
- **üìä InfoNCE per Group**: Single best positive vs negatives trong c√πng generation group  
- **‚ö° Seamless Integration**: T·ª± ƒë·ªông enable/disable d·ª±a tr√™n config

**Architecture:**
```
1 prompt ‚Üí N generations ‚Üí N embeddings + 1 prompt embedding
‚îú‚îÄ‚îÄ Supervised Contrastive: All correct answers = positive, wrong answers = negative  
‚îî‚îÄ‚îÄ InfoNCE: Best correct answer = positive, others = negative
```

```python
# Trong enhanced_trainer.py
class EnhancedGRPOTrainer(GRPOTrainer):
    def _compute_contrastive_loss(self, ...):
        # Per-group contrastive: all correct answers as positives
        for prompt_idx in range(num_prompts):
            positive_mask = rewards[prompt_idx] > 0  # All correct = positive
            # Contrastive loss within this generation group
        
    def _compute_infonce_loss(self, ...):
        # Per-group InfoNCE: single best positive
        for prompt_idx in range(num_prompts):
            best_positive = find_best_correct_shortest(group)
            # InfoNCE loss within this generation group
```

### 2. Length + Accuracy Reward

**M·ª•c ti√™u**: Gen ra c√¢u tr·∫£ l·ªùi ƒë√∫ng + ng·∫Øn g·ªçn.

**Implementation**: 

```python
def length_accuracy_reward(completions, solution, **kwargs):
    # Correct answers: reward ‚àù 1/length (shorter = better)
    # Wrong answers: penalty ‚àù length (longer = worse penalty)
    if is_correct:
        reward = 0.5 + 0.5 * (max_len - length) / (max_len - min_len)
    else:
        reward = -0.5 - 0.5 * (length - min_len) / (max_len - min_len)
```

### 3. X·ª≠ L√Ω Edge Cases

**Case 1**: Kh√¥ng c√≥ positive n√†o
- **Gi·∫£i ph√°p**: Ch·ªçn sample c√≥ reward cao nh·∫•t + length ng·∫Øn nh·∫•t l√†m positive

**Case 2**: Kh√¥ng c√≥ negative n√†o  
- **Gi·∫£i ph√°p**: D√πng majority voting (TTRL paper) ƒë·ªÉ t·∫°o consensus label

```python
def majority_voting_reward(completions, solution, **kwargs):
    # ƒê·∫øm t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa m·ªói ƒë√°p √°n
    answer_counts = Counter(all_answers)
    consensus_answer = answer_counts.most_common(1)[0][0]
    # Reward d·ª±a v√†o consensus + length
```

### 4. High Entropy Exploration

**V·∫•n ƒë·ªÅ**: C√¢u h·ªèi kh√≥ kh√¥ng sample ƒë∆∞·ª£c ƒë√°p √°n ƒë√∫ng.

**Gi·∫£i ph√°p**: D√πng entropy exploration (paper REASONING WITH EXPLORATION)

```python
def entropy_exploration_reward(completions, solution, difficulty_threshold=0.3):
    success_rate = correct_count / total_count
    if success_rate < difficulty_threshold:  # C√¢u kh√≥
        # Add entropy bonus ƒë·ªÉ khuy·∫øn kh√≠ch exploration
        reward += entropy_bonus
    # C√¢u d·ªÖ train b√¨nh th∆∞·ªùng
```

## üöÄ C√°ch S·ª≠ D·ª•ng

### 1. Quick Start v·ªõi Config:

```yaml
# Enhanced GRPO with Contrastive Learning
contrastive_weight: 0.5    # Set > 0 to enable contrastive loss
infonce_weight: 0.3        # Set > 0 to enable InfoNCE loss
temperature: 0.07          # Temperature for similarity computation

# Custom reward functions
reward_funcs:
  - "length_accuracy"      # Combined length + accuracy reward
  - "majority_voting"      # Handle cases with no ground truth
  - "entropy_exploration"  # High entropy for difficult questions
```

### 2. Ch·∫°y Training:

```bash
# V·ªõi contrastive learning
python -m open_r1.grpo --config example_config.yaml

# Ho·∫∑c disable contrastive learning (fallback to standard GRPO)
python -m open_r1.grpo --config standard_config.yaml \
  --contrastive_weight 0.0 --infonce_weight 0.0
```

### 3. Programmatic Usage:

```python
from open_r1.enhanced_trainer import EnhancedGRPOTrainer

# Automatically chooses Enhanced or Standard based on weights
trainer = EnhancedGRPOTrainer(
    model=model,
    reward_funcs=["length_accuracy", "entropy_exploration"],
    contrastive_weight=0.5,      # > 0 enables contrastive learning
    infonce_weight=0.3,          # > 0 enables InfoNCE
    contrastive_temperature=0.07,
    enable_length_reward_scaling=True,
    **standard_grpo_args
)
```

## Ki·∫øn Tr√∫c Implementation

```
open_r1/
‚îú‚îÄ‚îÄ custom_rewards.py          # Custom reward functions
‚îú‚îÄ‚îÄ custom_trainer.py          # Extended GRPO trainer
‚îú‚îÄ‚îÄ data_collator.py          # Data collator v·ªõi position tracking
‚îú‚îÄ‚îÄ grpo.py                   # Updated main script
‚îú‚îÄ‚îÄ configs.py                # Updated v·ªõi custom parameters
‚îî‚îÄ‚îÄ example_config.yaml       # Example configuration
```

## C√°c Reward Functions M·ªõi

1. **`length_accuracy`**: Combined length + accuracy reward
2. **`majority_voting`**: Majority voting cho consensus labels
3. **`entropy_exploration`**: High entropy exploration cho c√¢u kh√≥

## L·ª£i √çch

1. **Gi·∫£i quy·∫øt blackbox problem**: Extend thay v√¨ modify GRPO trainer
2. **Efficient reasoning**: C√¢u tr·∫£ l·ªùi ƒë√∫ng + ng·∫Øn g·ªçn
3. **Handle edge cases**: Majority voting khi kh√¥ng c√≥ label
4. **Adaptive exploration**: High entropy cho c√¢u kh√≥, b√¨nh th∆∞·ªùng cho c√¢u d·ªÖ
5. **Flexible configuration**: D·ªÖ d√†ng config v√† experiment

## Technical Details

- **Contrastive Loss**: Supervised contrastive v·ªõi multiple positives/negatives
- **InfoNCE Loss**: Cross-entropy v·ªõi 1 positive + nhi·ªÅu negatives  
- **Position Tracking**: Track prompt end v√† completion end positions
- **Reward Integration**: Combine v·ªõi existing GRPO rewards
- **Memory Efficient**: Kh√¥ng duplicate data, ch·ªâ th√™m metadata

V·ªõi implementation n√†y, b·∫°n c√≥ th·ªÉ train model ƒë·ªÉ generate efficient reasoning responses trong khi v·∫´n leverage ƒë∆∞·ª£c GRPO framework hi·ªán c√≥.
